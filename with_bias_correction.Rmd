---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---
# set working enviorement

```{r, message=FALSE, warning=FALSE}
setwd("C:/Users/kaner/OneDrive/university/second_degree/Thesis")

## Quantile regression models
library(quantreg) 
# library(quantregForest) 

## Data Manipulation
library(reshape) 
library(dplyr) 
library(reshape2)


## Visualization
library(ggplot2) 
library(grid) 
library(gridExtra)
library(cowplot)
library(lattice) 
library(knitr) 
library(latex2exp)
library(cowplot)
### Shiny App
library(shiny)
library(DT)

library(isotone)
library(sqldf)


library(foreach)
library(doParallel)
```

```{r}
colors = rainbow(20) ##colors for plot
```


# Auxiliary Functions


## randomize_data

```{r}
randomize_data <- function(n, error_distribution_type, error_distribution_params, 
                           x_mean, x_sd, rho = 0, p, true_coefs_x = NULL) {
  
  ## assuming same rho for all xs
  
  ## if we give p than all coefs are one, else we have to give the vector of coefs 
  if (is.null(p)) {
    p <- length(true_coefs_x) -1
  } else {
    true_coefs_x <- rep(1, p+1)
  }
  y <- rep(0,n)
  
  randomized_data <- data.frame(matrix(NA, nrow = n, ncol = p+1))
  
  ##  add each x_i
  for (i in 1:p) {
    
    x <- rnorm(n, x_mean, x_sd)
  
    if (error_distribution_type == "norm") {
      eps <- rnorm(n, mean = 0, sd = error_distribution_params[1])
    } 
    
    else if (error_distribution_type == "weibull") {
      eps <- rweibull(n, shape =  error_distribution_params[1], scale =  error_distribution_params[2])
    }
    
    else if (error_distribution_type == "t") {
      eps <- rt(n, df = error_distribution_params[1])
    }
    
    y <- y + true_coefs_x[i+1]*x + rho*x*eps
    
    randomized_data[,i] <- x
  }
  
  ## add intercept and error
  y <- true_coefs_x[1] + eps + y
  randomized_data[,p+1] <- y
  colnames(randomized_data)[p+1] <- "y"
  
  return (randomized_data)
}
```


## compute_conditional_quantile_line
```{r}
compute_conditional_quantile_line <- function(tau_true, error_distribution_type, 
                                              error_distribution_params, rho = 0, p, true_coefs_x = NULL) {

    
  if (is.null(p)) {
    p <- length(true_coefs_x) -1
  } else {
    true_coefs_x <- rep(1, p+1)
  }

  quan_line_coefficients <- data.frame(matrix(NA, nrow = 1, ncol = p+1))
  
  if (error_distribution_type == "norm") {
    epsilon_quantile <- qnorm(tau_true, mean = 0, sd = error_distribution_params[1])
  }
  else if (error_distribution_type == "weibull") {
    epsilon_quantile <- qweibull(tau_true, shape = error_distribution_params[1], scale = error_distribution_params[2]) 
  }
  else if (error_distribution_type == "t") {
    epsilon_quantile <- qt(tau_true, df = error_distribution_params[1]) 
  }
    
  
  quan_line_coefficients[,1] <- true_coefs_x[1] + epsilon_quantile
  for (i in 2:(p+1)) {
    quan_line_coefficients[,i] <- true_coefs_x[i] + rho * epsilon_quantile

  }
  
  return (quan_line_coefficients)
}
```


## compute_test_predictions_under_settings

```{r}
compute_test_predictions_under_settings <- function(n, error_distribution_type, error_distribution_params,
                                                n_test, test_data, x_mean, x_sd, rho = 0, p,true_coefs_x = NULL,
                                                bias_correction_val_size = 0.8, sample_size = 1000) {
  
  if (is.null(p)) {
    p <- length(true_coefs_x) -1
  } else {
    true_coefs_x <- rep(1, p+1)
  }
  
  
  test_predictions = data.frame(matrix(NA, nrow = n_test*(length(taus_wrong)+1), ncol = (sample_size + 3)))
  ## bias correction only for taus wrong
  test_predictions_bias_correction = data.frame(matrix(NA, nrow = n_test*(length(taus_wrong)), ncol = (sample_size + 3)))

  
  ## compute true line and its prediction on test set
  quan_line_coefficients <- 
  compute_conditional_quantile_line(tau_true, error_distribution_type, 
                                    error_distribution_params, true_coefs_x, rho = rho, p =p)

  true_quan_line_on_test <- 
    as.matrix(cbind(rep(1,n_test),(test_data %>% dplyr::select(-c(y))))) %*% c(t(quan_line_coefficients))


  ## compute models and predictions on test set
  for (sample in seq(1,sample_size)) {
  
    train_data <- randomize_data(n, error_distribution_type, error_distribution_params, 
                                 x_mean, x_sd, true_coefs_x, rho = rho, p=p) 
    
    ## first fit tau_true quantile model 
    fit_true <- rq(y ~ ., tau = tau_true, data = train_data)
    predict_true <- predict(fit_true, test_data)
    test_predictions[1:n_test,1] <- rep(tau_true,n_test)
    test_predictions[1:n_test,2] <- true_quan_line_on_test
    test_predictions[1:n_test,3] <- rep("true",n_test)
    test_predictions[1:n_test,(sample+3)] <- predict_true

    ## now fit taus_wrong quantile model
    for (i in 1:length(taus_wrong)) {
         fit_wrong <- rq(y ~ ., tau = taus_wrong[i], data = train_data)
         predict_wrong <- predict(fit_wrong, test_data)
         
         test_predictions[((n_test*i)+1):(n_test*(i+1)),1] <- rep(taus_wrong[i],n_test)
         test_predictions[((n_test*i)+1):(n_test*(i+1)),2] <- true_quan_line_on_test
         test_predictions[((n_test*i)+1):(n_test*(i+1)),3] <- rep("no_correction",n_test)
         test_predictions[((n_test*i)+1):(n_test*(i+1)),(sample+3)] <- predict_wrong
         
         
         ## bias_correction
         len <- dim(train_data)[1]
         bs <- 50
         step_mean_test <- rep(0, length(predict_wrong))

         for (b in 1:bs) {
  
            small_train_data <- sample_n(train_data, (bias_correction_val_size*len), replace = F)
            small_eval_data <- sqldf("select * from train_data except select * from small_train_data")
          
            fit_wrong_small_train <- rq(y ~ ., tau = taus_wrong[i], data = small_train_data)
            predict_wrong_small_train_on_val <- predict(fit_wrong_small_train, small_eval_data)
          
            res_p <- gpava(z = predict_wrong_small_train_on_val, y = small_eval_data$y, 
                       solver = weighted.fractile, p = tau_true)
          
            step_function <- stepfun(x = sort(res_p$z),y = sort(c(min(res_p$x),res_p$x)))
            
            step_mean_test <- step_mean_test + step_function(predict_wrong)
            
         }

  
         test_predictions_bias_correction[((n_test*(i-1))+1):(n_test*(i)),1] <- rep(taus_wrong[i],n_test)
         test_predictions_bias_correction[((n_test*(i-1))+1):(n_test*(i)),2] <- true_quan_line_on_test
         test_predictions_bias_correction[((n_test*(i-1))+1):(n_test*(i)),3] <- rep("bias_correction",n_test)
         test_predictions_bias_correction[((n_test*(i-1))+1):(n_test*(i)),(sample+3)] <- (step_mean_test/bs)

    }
  
  }

  colnames(test_predictions)[3] <- "method"
  colnames(test_predictions)[2] <- "true_quantile_in_x"
  colnames(test_predictions)[1] <- "quantile"
  colnames(test_predictions_bias_correction)[3] <- "method"
  colnames(test_predictions_bias_correction)[2] <- "true_quantile_in_x"
  colnames(test_predictions_bias_correction)[1] <- "quantile"
  

  return (rbind(test_predictions,test_predictions_bias_correction))

}
```

## compute_Bias_Var

```{r}

compute_Bias_Var <- function(test_predictions, n_test) {
  
  errors = data.frame()

  for (i in 0:(length(taus_wrong)*2)){
    
    ## remove the column of the number of quantile
    quantile_test_predictions_data <- 
      test_predictions[((n_test*i)+1):(n_test*(i+1)),] %>% 
      dplyr::select(-c(1))
    quantile <- test_predictions[((n_test*i)+1),1]
    
    quantile_test_predictions_data <- quantile_test_predictions_data %>% 
      mutate("mean_pred_in_x" = apply(quantile_test_predictions_data %>% 
                                        dplyr::select(-c(true_quantile_in_x, method)), 1,mean))
  
    Squared_Bias <- (quantile_test_predictions_data$true_quantile_in_x - 
                     quantile_test_predictions_data$mean_pred_in_x)^2

    Var <- quantile_test_predictions_data %>% 
            dplyr::select(-c(true_quantile_in_x,mean_pred_in_x,method)) %>% 
            apply(2,function (col) {(quantile_test_predictions_data$mean_pred_in_x - col)^2}) %>% 
            apply(1,mean)
  
    MSE <- quantile_test_predictions_data %>% 
          dplyr::select(-c(true_quantile_in_x,mean_pred_in_x,method)) %>% 
          apply(2,function (col) {(quantile_test_predictions_data$true_quantile_in_x - col)^2}) %>% 
          apply(1,mean)
  
    errors_quantile <- data.frame("quantile"= quantile, 
                         "true_quan" = quantile_test_predictions_data$true_quantile_in_x,
                         "method" = quantile_test_predictions_data$method,
                         "Squared_Bias" = Squared_Bias,
                         "Var" = Var,
                         "MSE" = MSE)
    
    errors <- rbind(errors, errors_quantile)}
    
  return (errors)
  }
```



# Settings for all models

```{r}
## model settings
ps <- c(1,8,15)
rhos <- c(0,1)

tau_true <- 0.99
taus_wrong <- c(0.95,0.97)

## sample settings
n_test <- 2000
ns <- c(125,250,500,1000)

x_mean <- 1
x_sd <- 0.01

```



# Normal Errors

$$\begin{cases}
y^{(1)}=a+bx+e_{N}\\
\\
e_{_{N}}\sim N(0,\sigma)
\end{cases}$$

```{r}
## distribution settings
sigmas <- c(0.001, 0.01, 0.1)
```


```{r}
# errors_normal <- data.frame()
# 
# for (p in ps) {
#    for (rho in rhos) {
# 
#     for (sigma in sigmas) {
# 
#     test_data <- randomize_data(n_test, error_distribution_type = "norm",
#                                 error_distribution_params = sigma,
#                                 x_mean = x_mean, x_sd = x_sd, p = p , rho = rho)
# 
#     for (n in ns) {
# 
# 
#          cat("p: " ,p , "rho: ", rho, "sigma: ", sigma, ", n:", n, "\n")
# 
#          test_predictions <-
#            compute_test_predictions_under_settings(n, error_distribution_type = "norm",
#                                                error_distribution_params = sigma,
#                                                n_test = n_test, test_data = test_data,
#                                                x_mean = x_mean, x_sd = x_sd, rho = rho, p = p,
#                                                bias_correction_val_size = 0.7, sample = 100)
# 
#          errors <- compute_Bias_Var(test_predictions = test_predictions, n_test = n_test)
# 
#          errors_normal <- rbind(errors_normal, data.frame("p" = p,
#                                                           "rho"= rho,
#                                                           "n" = n,
#                                                           "sigma" = sigma,
#                                                           errors))
# 
# 
# 
#    }
#  }
# }
# 
# }
# 
# write.csv(errors_normal, "outputs/errors_normal.csv", row.names=FALSE)

```


```{r}
# run_par <- function(p, rho, sigma) {
#     errors_par <- data.frame()
#     
#     test_data <- randomize_data(n_test, error_distribution_type = "norm",
#                                 error_distribution_params = sigma,
#                                 x_mean = x_mean, x_sd = x_sd,p = p, rho = rho)
#      for (n in ns) {
# 
# 
#      cat("p: " ,p, "rho: ", rho, "sigma: ", sigma, ", n:", n, "\n")
# 
#          test_predictions <-
#            compute_test_predictions_under_settings(n, error_distribution_type = "norm",
#                                                error_distribution_params = sigma,
#                                                n_test = n_test, test_data = test_data,
#                                                x_mean = x_mean, x_sd = x_sd, rho = rho,p=p,
#                                                bias_correction_val_size = 0.7, sample = 100)
# 
#          errors <- compute_Bias_Var(test_predictions = test_predictions, n_test = n_test)
# 
#          
#          
#          errors_par <- rbind(errors_par, data.frame("p" = p,
#                                                           "rho"= rho,
#                                                           "n" = n,
#                                                           "sigma" = sigma,
#                                                           errors))
#         
#      }
#     return (errors_par)
#   
# }
```


```{r}
# #setup parallel backend to use many processors
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)
# 
# packages = c("dplyr", "quantreg", "isotone", "sqldf", "knitr", "reshape", "reshape2")
# 
# errors_normal <- data.frame()
# strptime(Sys.time(), "%Y-%m-%d %H:%M:%S")
# errors_normal <-
#   foreach(p = ps, .packages=packages, .combine=rbind) %:%
#   foreach(rho = rhos, .packages=packages, .combine=rbind) %:%
#   foreach(sigma = sigmas, .packages=packages, .combine=rbind) %dopar% {
#       errors = run_par(p, rho, sigma)
#       errors
#   }
# write.csv(errors_normal, "outputs/errors_normal.csv", row.names=FALSE)
# strptime(Sys.time(), "%Y-%m-%d %H:%M:%S")
# stopCluster(cl)
```



```{r}
errors_normal <- read.csv("outputs/errors_normal.csv", sep =",")
```



# Weibull errors

$$\begin{cases}
y^{(3)}=a+bx+e_{w}\\
\\
e_{w}\sim weibull(shape=a,scale=b)
\end{cases}$$

```{r}
## distribution settings
shapes <- c(0.5,1,2)
scales <- c(0.5,1,2)
```

```{r}
# errors_weibull <- data.frame()

# for (p in ps) {
#   for (rho in rhos) {
#     for (shape in shapes) {
#       for (scale in scales) {
# 
#         test_data <- randomize_data(n_test, error_distribution_type = "weibull",
#                       error_distribution_params = c(shape, scale),
#                       x_mean = x_mean, x_sd = x_sd,rho = rho, p=p)
# 
#         for (n in ns) {
# 
#             cat("p: " ,p, "rho: ", rho,  "shape: ", 
#                 shape, " scale:", scale, ", n:", n, "\n")
#   
#             test_predictions <-
#               compute_test_predictions_under_settings(n, error_distribution_type = "weibull",
#                                                   error_distribution_params = c(shape, scale),
#                                                   n_test = n_test, test_data = test_data,
#                                                   x_mean = x_mean, x_sd = x_sd, rho = rho, p =p ,
#                                                   bias_correction_val_size = 0.7, sample = 100)
#   
#             errors <- compute_Bias_Var(test_predictions = test_predictions, n_test = n_test)
#   
#             errors_weibull <- rbind(errors_weibull,
#                                     data.frame("p" = p,
#                                                "rho"= rho,
#                                                "n" = n,
#                                                "shape" = shape,
#                                                "scale" = scale,
#                                                errors))
# 
#             
# 
#         }
#       }
#     }
#   }
# 
# }
# 
# 
# write.csv(errors_weibull, "outputs/errors_weibull.csv", row.names=FALSE)

```




```{r}
# run_par <- function(p, rho, shape, scale) {
#     errors_par <- data.frame()
#     
#     test_data <- randomize_data(n_test, error_distribution_type = "weibull",
#                                 error_distribution_params = c(shape, scale),
#                                 x_mean = x_mean, x_sd = x_sd,p = p, rho = rho)
#      for (n in ns) {
# 
#          test_predictions <-
#            compute_test_predictions_under_settings(n, error_distribution_type = "weibull",
#                                                error_distribution_params = c(shape, scale),
#                                                n_test = n_test, test_data = test_data,
#                                                x_mean = x_mean, x_sd = x_sd, rho = rho, p = p,
#                                                bias_correction_val_size = 0.7, sample = 100)
# 
#          errors <- compute_Bias_Var(test_predictions = test_predictions, n_test = n_test)
# 
#          errors_par <- rbind(errors_par, data.frame("p" = p,
#                                                           "rho"= rho,
#                                                           "n" = n,
#                                                           "shape" = shape,
#                                                           "scale" = scale,
#                                                           errors))
#         
#      }
#     return (errors_par)
#   
# }
```


```{r}
# #setup parallel backend to use many processors
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)
# 
# packages = c("dplyr", "quantreg", "isotone", "sqldf", "knitr", "reshape", "reshape2")
# 
# strptime(Sys.time(), "%Y-%m-%d %H:%M:%S")
# 
# errors_weibull <- data.frame()
# errors_weibull <- 
#   foreach(p = ps, .packages=packages, .combine=rbind) %:% 
#   foreach(rho = rhos, .packages=packages, .combine=rbind) %:%  
#   foreach(shape = shapes, .packages=packages, .combine=rbind) %:% 
#   foreach(scale = scales, .packages=packages, .combine=rbind) %dopar% {
#       errors = run_par(p, rho, shape, scale)
#       errors
#   }
# 
# strptime(Sys.time(), "%Y-%m-%d %H:%M:%S")
# write.csv(errors_weibull, "outputs/errors_weibull.csv", row.names=FALSE)
# 
# stopCluster(cl)    
```

```{r}
errors_weibull <- read.csv("outputs/errors_weibull.csv", sep =",")
```


# t-distribution errors

$$\begin{cases}
y^{(1)}=a+bx+e_{t}\\
\\
e_{_{t}}\sim t(df)
\end{cases}$$

```{r}
## distribution settings
dfs <- c(1, 2, 3, 10)
```

```{r}
# errors_t <- data.frame()
# 
# for (p in ps) {
#   for (rho in rhos) {
#     for (df in dfs) {
#         test_data <- randomize_data(n_test, error_distribution_type = "t", error_distribution_params = df,
#                           x_mean = x_mean, x_sd = x_sd, rho=rho, p=p)
#         for (n in ns) {
#             
#           cat( "p: " ,p, "rho: ", rho,  "df: ", df, ", n:", n, "\n")
#             
#             test_predictions <-
#               compute_test_predictions_under_settings(n, error_distribution_type = "t",
#                                                       error_distribution_params = df,
#                                                       n_test = n_test, test_data = test_data,
#                                                       x_mean = x_mean, x_sd = x_sd, rho = rho, p=p,
#                                                       bias_correction_val_size = 0.7, sample = 100)
#   
#             errors <- compute_Bias_Var(test_predictions = test_predictions, n_test = n_test)
#   
#             errors_t <- rbind(errors_t,
#                                   data.frame("p" = p,
#                                              "rho"= rho,
#                                              "n" = n,
#                                              "df" = df,
#                                              errors))
#           
#         }
#     }
#   }
# }

# write.csv(errors_t, "outputs/errors_t.csv", row.names=FALSE)
```



```{r}
# run_par <- function(p, rho, df) {
#     errors_par <- data.frame()
#     
#     test_data <- randomize_data(n_test, error_distribution_type = "t",
#                                 error_distribution_params = df,
#                                 x_mean = x_mean, x_sd = x_sd,p = p, rho = rho)
#      for (n in ns) {
# 
# 
#      cat("p: " ,p, "rho: ", rho, "df: ", df, ", n:", n, "\n")
# 
#          test_predictions <-
#            compute_test_predictions_under_settings(n, error_distribution_type = "t",
#                                                error_distribution_params = df,
#                                                n_test = n_test, test_data = test_data,
#                                                x_mean = x_mean, x_sd = x_sd, rho = rho,p=p,
#                                                bias_correction_val_size = 0.7, sample = 100)
# 
#          errors <- compute_Bias_Var(test_predictions = test_predictions, n_test = n_test)
# 
#          
#          
#          errors_par <- rbind(errors_par, data.frame("p" = p,
#                                                           "rho"= rho,
#                                                           "n" = n,
#                                                           "df" = df,
#                                                           errors))
#         
#      }
#     return (errors_par)
#   
# }
```


```{r}
# #setup parallel backend to use many processors
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# 
# registerDoParallel(cl)
# 
# packages = c("dplyr", "quantreg", "isotone", "sqldf", "knitr", "reshape", "reshape2")
# 
# errors_t <- data.frame()
# strptime(Sys.time(), "%Y-%m-%d %H:%M:%S")
# errors_t <- 
#   foreach(p = ps, .packages=packages, .combine=rbind) %:% 
#   foreach(rho = rhos, .packages=packages, .combine=rbind) %:%  
#   foreach(df = dfs, .packages=packages, .combine=rbind) %dopar% {
#       errors = run_par(p, rho, df)
#       errors
#   }
# write.csv(errors_t, "outputs/errors_t.csv", row.names=FALSE)
# strptime(Sys.time(), "%Y-%m-%d %H:%M:%S")
# stopCluster(cl)    
```

```{r}
errors_t <- read.csv("outputs/errors_t.csv", sep =",")
```

# Application


```{r, message=FALSE, warning=FALSE}

normal_melted_data_iid <- 
  errors_normal %>% 
  dplyr::filter(rho==0) %>% 
  melt(id.vars = seq(1,7)) %>% 
  mutate(quntile_final = ifelse(method == "bias_correction", paste0(quantile,"_corrected"), quantile)) %>% 
  select(-c(true_quan))


normal_melted_data_het <- 
  errors_normal %>% 
  dplyr::filter(rho==1) %>% 
  melt(id.vars = seq(1,7)) %>% 
  mutate(quntile_final = ifelse(method == "bias_correction", paste0(quantile,"_corrected"), quantile))%>% 
  select(-c(true_quan))


weibull_melted_data_iid <- 
  errors_weibull %>% 
  dplyr::filter(rho==0) %>% 
  melt(id.vars = seq(1,8)) %>% 
  mutate(quntile_final = ifelse(method == "bias_correction", paste0(quantile,"_corrected"), quantile))%>% 
  select(-c( true_quan))


weibull_melted_data_het <- 
  errors_weibull %>% 
  dplyr::filter(rho==1) %>% 
  melt(id.vars = seq(1,8)) %>% 
  mutate(quntile_final = ifelse(method == "bias_correction", paste0(quantile,"_corrected"), quantile))%>% 
  select(-c(true_quan))


t_melted_data_iid <- 
  errors_t %>% 
  dplyr::filter(rho==0) %>% 
  melt(id.vars = seq(1,7)) %>% 
  mutate(quntile_final = ifelse(method == "bias_correction", paste0(quantile,"_corrected"), quantile))%>% 
  select(-c(true_quan))


t_melted_data_het <- 
  errors_t %>% 
  dplyr::filter(rho==1) %>% 
  melt(id.vars = seq(1,7)) %>% 
  mutate(quntile_final = ifelse(method == "bias_correction", paste0(quantile,"_corrected"), quantile))%>% 
  select(-c(true_quan))


```


## ui 

```{r}
ui <- fluidPage(
  
  headerPanel("My Shiny Thesis"),
  submitButton("Update View", icon("angellist")),

  # Sample parameters
  verticalLayout(
    
    splitLayout(
  
        radioButtons("distribution", h5("Distribution"), 
                    choices = list("Normal" = "norm","Student's t"="t", "Weibull" = "weibull"),
                    selected = "norm"),
        
        radioButtons("rho", h5("Rho"), 
                    choices = rhos,
                    selected = 0),
        
        radioButtons("p", h5("P"), 
                    choices = ps,
                    selected = 1),

        
        checkboxGroupInput("taus_wrong", h5("Wrong quantiles"), 
                    choices = taus_wrong,
                    selected = 0.97),
        
        radioButtons("method", h5("Methods to show"), 
                    choices = list("Modeling wrong quantiles",
                                   "Bias Correction", 
                                   "Show all"),
                    selected = "Modeling wrong quantiles"),
        
        checkboxGroupInput("n", h5("Train set size"),
                       choices = ns, selected = ns))),
  
  ## distribution parameters
    sidebarPanel(
            width = 3,
            h3("Distribution Parameters"),
            radioButtons("sigma", h5("Sigma"),
                 choices = sigmas, selected = 0.001),
            radioButtons("df", h5("Degreed of freedom"),
                 choices = dfs, selected = 1),
            
            splitLayout(

            radioButtons("shape", h5("Shape"),
                 choices = shapes, selected = 1),
            radioButtons("scale", h5("Scale"),
                 choices = scales, selected = 1)
            
            )

     ),

  
  ## tabs of different plots
   mainPanel(
     
     tabsetPanel (
    
            ## Tab 1 - plot the errors over differnt parameters
               tabPanel ("Simulation Errors", fluid = "TRUE",
               plotOutput(outputId = "boxplot")
               ),
    
            ## tab 2
              tabPanel ("Simulation Residuls Distribution", fluid = "TRUE",
              plotOutput(outputId = "density")
               )
  
  
     )
  )
)
```



## server 

```{r}
server <- function(input, output) {
  
  distribution_data <- reactive({
    
        if (input$distribution=='weibull') {
          if (input$rho==0){
            chosen_distribution <- weibull_melted_data_iid
          } else {
            chosen_distribution <- weibull_melted_data_het
          }
        } else if (input$distribution=='t'){
          if (input$rho==0){
            chosen_distribution <- t_melted_data_iid
          } else {
            chosen_distribution <- t_melted_data_het
          }
        } else if (input$rho==0) {
          chosen_distribution <- normal_melted_data_iid 
        } else {
          chosen_distribution <- normal_melted_data_het 
        }
        
        return (data.frame(chosen_distribution))
  })


  
   filtered_data_by_params <- reactive({
          
          filtered_data_param = distribution_data()
          
          if (input$distribution == "norm") {
              filtered_data = filtered_data_param %>% 
                          dplyr::filter((n %in% input$n) & (sigma == input$sigma) & 
                                          (quantile %in% c(tau_true, input$taus_wrong)) &
                                          (rho == input$rho) & (p == input$p))
          } else if (input$distribution == "weibull") {
              filtered_data = filtered_data_param %>% 
                          dplyr::filter((n %in% input$n) & (shape == input$shape) & (scale == input$scale) &
                                        (quantile %in% c(tau_true, input$taus_wrong)) &
                                        (rho == input$rho) & (p == input$p))
          } else if (input$distribution == "t") {
              filtered_data = filtered_data_param %>% 
                          dplyr::filter((n %in% input$n) & (df == input$df) & 
                                        (quantile %in% c(tau_true, input$taus_wrong)) &
                                        (rho == input$rho) & (p == input$p))}
         
          return (data.frame(filtered_data))
     
   })
   
    method_data <- reactive({
    
        filtere_data_by_param = filtered_data_by_params()
      
        if (input$method=="Modeling wrong quantiles") {
           filtere_data_by_param <- filtere_data_by_param %>% dplyr::filter(method!="bias_correction")
        } else if (input$method=="Bias Correction") {
           filtere_data_by_param <- filtere_data_by_param %>% dplyr::filter(method!="no_correction")
        } else {
           filtere_data_by_param <- filtere_data_by_param
        }
        
        return (data.frame(filtere_data_by_param))
  })
   
     
   errors_dist_data <- reactive({
     
      data <- filtered_data_by_params() %>% 
           filter(variable == "MSE" & quantile == tau_true & 
                    n == max(input$n) & p==min(input$p) & rho==min(input$rho))
     
      if (input$distribution == "norm") {
        dist <- rnorm(data$n, sd = data$sigma)
      } else if (input$distribution == "weibull") {
        dist <- rweibull(data$n, shape = data$shape, scale = data$scale)
      } else if (input$distribution == "t") {
        dist <- rt(data$n, df =  data$df)
      } 
      return (data_frame(x = dist))
   })
 
   # y_x_data <- reactive({
   #   ## show y~x for p=1
   #  ## not working
   #   
   #   error_distribution_params_input <- ifelse(input$distribution == "norm", 
   #                                       max(input$sigma), 
   #                                       ifelse(input$distribution == "weibull",
   #                                              c(max(input$shape), max(input$scale)),
   #                                              max(input$df)))
   # 
   #   randomized_data <- randomize_data(n = max(input$n), 
   #                          error_distribution_type = input$distribution, 
   #                          error_distribution_params = error_distribution_params_input, 
   #                          x_mean = x_mean, x_sd = x_sd, rho = max(input$rho), p = 1)
   # 
   #   return (randomized_data)
   #   
   # })
  
     output$density <- renderPlot({

        ggplot(errors_dist_data(), aes(x)) +
        geom_histogram(col = "cornflowerblue", 
                       fill = "cornflowerblue", alpha = 0.7, bins = 100) +
        labs(x = "residual value", y = "count") +
        theme_grey()  
        
        # grid.arrange(hist, dist, ncol = 2)
     })
     
  
    output$boxplot <- renderPlot({
    
        ggplot(method_data(),
         aes(x = as.factor(n), y = log(value+1),  
             fill = as.factor(quntile_final), col = as.factor(quntile_final)))+
        geom_boxplot(col = "#585858", outlier.size = 0.1)+
        facet_wrap(~ variable) + #scales="free"
        labs(x = "n", y = "log(value+1)", fill = "Quantile") +
        # scale_colour_manual(values = c("0.95" = "#9BC01C", 
        #                                "0.95_corrected" = "#C3E05E",
        #                                "0.97" = "#F52549", 
        #                                "0.97_corrected" = "#FA6775",
        #                                "0.99" = "#f48d38"))+
        # scale_fill_manual(values = c("0.95" = "#9BC01C", 
        #                                "0.95_corrected" = "#C3E05E",
        #                                "0.97" = "#F52549", 
        #                                "0.97_corrected" = "#FA6775",
        #                                "0.99" = "#f48d38"))+
        scale_fill_brewer(palette="Paired") +
        theme_grey() +
        theme(legend.position = "bottom")

     })
    


}


```

## run app

```{r}
shinyApp(ui = ui, server = server)
```


## summary of interesting cases


## Normal Distribution

```{r, fig.height=12, fig.width=12}
iid <- ggplot(normal_melted_data_iid %>% 
       dplyr::filter((p %in% c(15)) & (sigma == 0.1)  
                     & (quantile %in% c(0.97,0.99))
                     & (n %in% ns)),
       aes(x = as.factor(n), y = log(value+1), fill = as.factor(quntile_final)))+
geom_boxplot(inherit.aes = T, 
             col = "#585858", outlier.size = 0.15)+
facet_grid(~variable, scales = "free") + 
labs(x = "n", y = "log(value+1)", fill = "Method") +
scale_fill_brewer(palette="Paired") +
 # theme_light() +
theme_grey() +
theme(legend.position = "None")

  
het <- 
  ggplot(normal_melted_data_het %>% 
       dplyr::filter((p %in% c(15)) & (sigma == 0.1)  
                     & (quantile %in% c(0.97,0.99))
                     & (n %in% ns)),
       aes(x = as.factor(n), y = log(value+1), fill = as.factor(quntile_final)))+
geom_boxplot(inherit.aes = T, 
             col = "#585858", outlier.size = 0.15)+
facet_grid(~variable, scales = "free") + 
labs(x = "n", y = "log(value+1)", fill = "Method") +
scale_fill_brewer(palette="Paired") +
 # theme_light() +
theme_grey() +
theme(legend.position = "bottom")


grid.arrange(arrangeGrob(iid, top="Location model"), 
             arrangeGrob(het, top="Scale-location model"), 
             nrow = 2, heights = c(0.47,0.53))
```

```{r, fig.height=12, fig.width=12}
ggplot(t_melted_data_iid %>% 
       dplyr::filter((p %in% c(15)) & (df %in% dfs)  
                     & (quantile %in% c(0.97,0.99))
                     & (n %in% ns)),
       aes(x = as.factor(n), y = log(value+1), fill = as.factor(quntile_final)))+
geom_boxplot(inherit.aes = T, 
             col = "#585858", outlier.size = 0.15)+
facet_grid(df~variable, scales = "free") + 
labs(x = "n", y = "log(value+1)", fill = "Method") +
scale_fill_brewer(palette="Paired") +
 # theme_light() +
theme_grey() +
theme(legend.position = "None")

```


```{r, fig.height=8, fig.width=12}
ggplot(weibull_melted_data_iid %>% 
       dplyr::filter((p == 15) & (shape %in% shapes) & (scale ==1)  
                     & (quantile %in% c(0.97,0.99))
                     & (n %in% ns) ),
       aes(x = as.factor(n), y = log(value+1), fill = as.factor(quntile_final)))+
geom_boxplot(inherit.aes = T, 
             col = "#585858", outlier.size = 0.15)+
facet_grid(shape~variable, scales = "free") + 
labs(x = "n", y = "log(value+1)", fill = "Method") +
scale_fill_brewer(palette="Paired") +
 # theme_light() +
theme_grey() +
theme(legend.position = "None")

```


# Bias correction

## isotonic

```{r}
df <- 2

## model settings
true_coefs_xs <- list(rep(1,2), rep(1,6) ,rep(1,11), rep(1,21))
true_coefs_x <- rep(1,11)
rho <- 0

tau_true <- 0.99
taus_wrong <- c(0.97)

## sample settings
n_test <- 3000
ns <- c(125,250,500,1000,2000)
n <- 250
x_mean <- 1
x_sd <- 0.01

error_distribution_type <- "t"
error_distribution_params <- df
```


```{r}
test_data <- randomize_data(n_test, error_distribution_type = error_distribution_type, error_distribution_params = error_distribution_params, x_mean = x_mean, x_sd = x_sd, true_coefs_x = true_coefs_x, rho=rho)


train_data <- randomize_data(n, error_distribution_type, error_distribution_params, 
                             x_mean, x_sd, true_coefs_x, rho = rho) 

quan_line_coefficients <- compute_conditional_quantile_line(tau_true, error_distribution_type, error_distribution_params, true_coefs_x, rho = rho)

test_predictions <-
  compute_test_predictions_under_settings(n, error_distribution_type = "t",
                                          error_distribution_params = df,
                                          n_test = n_test, test_data = test_data,
                                          x_mean = x_mean, x_sd = x_sd, rho = rho)

errors <- cbind(method = "qr",
                compute_Bias_Var(test_predictions = test_predictions, n_test = n_test))


test_predictions_bias_correction <- 
  compute_test_predictions_under_settings(n, error_distribution_type = "t",
                                          error_distribution_params = df,
                                          n_test = n_test, test_data = test_data,
                                          x_mean = x_mean, x_sd = x_sd, rho = rho,
                                          bias_correction = T, val_size = 0.8)


errors_bias_corretion <- cbind(method = "bias_correction",
  compute_Bias_Var(test_predictions = test_predictions_bias_correction, n_test = n_test)) 

all_errors <- rbind(errors, errors_bias_corretion)

```

```{r}
ggplot(all_errors %>% melt(id.vars = c(1,2,3)), 
       aes(x = quantile, y = log(value+1), group = as.factor(quantile)))+
  geom_boxplot() +
  facet_grid(variable~method) +
  theme_gray()

```


## How to know when does the bias correction help


### iid errors

```{r}
normal_melted_data_iid %>% 
  group_by(p,rho,n,sigma,quantile,method,variable) %>% 
  summarise(median = median(value),
            mean = mean(value),
            min = min(value), max = max(value),
            sd = sd(value)) %>% 
  filter(variable == "MSE" & quantile %in% c(0.97,0.99) & p>1) %>% 
  select(-c(variable)) %>% 
  dcast(p+rho+n+sigma ~ paste0('method_',method), value.var = 'median') %>% 
  mutate("is_bias_correction_good" = 
           ifelse(method_bias_correction < method_true, 1, 0),
         "is_modeling_wrong_quantile_good" = 
           ifelse(method_no_correction < method_true, 1, 0)) %>% 
  select(-c(method_bias_correction, method_no_correction, method_true)) %>% 
  arrange(-is_bias_correction_good)
```


```{r}
t_melted_data_iid %>% 
  group_by(p,rho,n,df,quantile,method,variable) %>% 
  summarise(median = median(value),
            mean = mean(value),
            min = min(value), max = max(value),
            sd = sd(value)) %>% 
  filter(variable == "MSE" & quantile %in% c(0.97,0.99) & p>1) %>% 
  select(-c(variable)) %>% 
  dcast(p+rho+n+df ~ paste0('method_',method), value.var = 'median') %>% 
  mutate("is_bias_correction_good" = 
           ifelse(method_bias_correction < method_true, 1, 0),
         "is_modeling_wrong_quantile_good" = 
           ifelse(method_no_correction < method_true, 1, 0)) %>% 
  select(-c(method_bias_correction, method_no_correction, method_true)) %>% 
  arrange(-is_bias_correction_good)
```




```{r}
distribution_type = "weibull"
scale = 0.5
location = 1
rho=0

train_data <- randomize_data(1000, distribution_type, c(scale,location),
                             x_mean, x_sd, true_coefs_x, rho = rho)
    
model <- rq(y~., tau= 0.99, data = train_data)

kernel <- function(x) {
  if (abs(x)<=1) {return (0.5)}
  else (return (0))
}

train_data$y_pred <- predict(model)
train_data$res <- train_data$y - train_data$y_pred
train_data$ker <- sapply(train_data$res / bandwidth, function (x) {kernel(x)})


J_het = matrix(0, nrow=15, ncol=15)
for (i in 1:1000) {
  mat = as.vector(t(train_data[i,] %>% select(-c(y,y_pred,res,ker)))) %*% 
    as.matrix(train_data[i,] %>% select(-c(y,y_pred,res,ker)))
  
  J_het = J_het + mat
  
}
matrix_sparsity_estimator = J_het / (1000*bandwidth)


iid_estimator = (quantile(train_data$y,0.99+bandwidth) - quantile(train_data$y,0.99-bandwidth))/(2*bandwidth)

final_matrix_estimator  = solve(matrix_sparsity_estimator)

summary(model, se = "nid")


```

